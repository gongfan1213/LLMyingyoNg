![image](https://github.com/user-attachments/assets/5ae1c194-5a56-4551-8797-e2ad56a6f2bb)



### 图6-3 Llama-2-7b-chat-hf微调（续）

**表6-3 Llama-2-7b-chat-hf微调过程情况**

|类别|参数|参数值|
| ---- | ---- | ---- |
|GPU|GPU内存占用|14.5GB|
| |GPU利用率|100%|
| |GPU温度|79℃（正常）|
|微调过程|总步数|3108步|
| |已进行步数|3步|
| |剩余时间|14h30min|
| |目前loss值|4.09|

#### 6.2.6 PEFT微调模型测试
使用PEFT微调方法得到的模型文件在output/PEFT/model目录下，由adapter_config.json和adapter_model.safetensors组成。adapter_config.json是微调模型的配置信息，记录了原始模型位置、微调策略等；adapter_model.safetensors则是在增量预训练阶段或有监督微调阶段产生的适配器文件，相当于针对原始权重的一个补丁。装载PEFT微调模型进行Demo测试，需要新建一个prompt.txt的文本文件，将测试问题写入这个文件中。测试命令如下。
```bash
python recipes/inference/local_inference/inference.py \
./dataroot/models/NousResearch/Llama-2-7b-chat-hf \
--base_model output/PEFT/model \
--prompt_file prompt.txt --safety_model False \
--enable_salesforce_content_safety False \
--quantization --max_new_tokens 200
```

#### 6.2.7 模型合并

为了便于分发和运行，可以把PEFT微调得到的模型与原始模型进行合并，形成新的模型。合并模型用merge_lora_weights.py脚本完成。

在执行脚本前，要将原始模型的generation_config.json文件删除或改名，以防止出现“ValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration”的错误（说明generation_config.json与PEFT冲突），合并命令如下。

```bash
python recipes/inference/model_servers/hf_text_generation_inference/merge_lora_weights.py \
--base_model./dataroot/models/NousResearch/Llama-2-7b-chat-hf \
--peft_model output/PEFT/model --output_dir output/merged/model
```

#### 6.2.8 合并后模型测试

上述模型合并后，将在output/merged/model目录下生成新的模型，用以下命令测试。

```bash
python recipes/inference/local_inference/inference.py \
./output/merged/model \
--prompt_file prompt.txt --peft_model output/PEFT/model \
--enable_salesforce_content_safety False \
--quantization --max_new_tokens 200
```

### 6.3 Gemma微调
对于Hugging Face格式的Gemma模型，可以采用Transformers库的SFTTrainer方法进行微调。即使对于2B或7B这些中等参数规模的语言模型来说，全量参数微调也会占用大量GPU内存和算力资源，再采用Gemma官方推荐的Colab或Kaggle开发运行平台，成本较高。如果只用于学习或实验，那么参数高效模型微调是一种低成本的微调方案，且效果尚可。

#### 6.3.1 微调方法介绍

PEFT的微调策略有LoRA、LLaMA Adapter、Prefix-Tuning，本节采用的是LoRA方式。LoRA是一种用于大语言模型的参数高效微调技术，这种微调方案只针对模型参数的一小部分进行微调，通过冻结原始模型并只训练被分解为低秩矩阵的适配器层，以此提高微调效率，减少灾难性遗忘等问题。

在本节实践中，根据Hugging Face针对Gemma进行PEFT微调提出的一种方案完善了源代码，从而更好地对Gemma-2B模型进行微调，过程中所用数据来自公开的“名人名言”数据集（Abirate/english-quotes）。在模型微调前针对一句名言“Imagination is more important than knowledge. Knowledge is limited. Imagination encircles the world”中的一部分“Imagination is more”进行补全，查看推理效果。模型微调后，可看到推理的结果会有变化，更接近于“名人名言”数据集的语料数据。

#### 6.3.2 微调环境准备

1. **建立Python虚拟环境**

```bash

# 建立一个Python3.10虚拟环境
conda create -n gemma python=3.10 -y
conda activate gemma
mkdir gemma-2b
cd gemma-2b
```
2. **安装依赖库**

先新建一个库依赖列表文件requirements.txt，内容如下。
```
torch==2.0.1
transformers==4.38.1
accelerate==0.27.2
bitsandbytes==0.42.0
trl==0.7.11
peft==0.8.2
```


然后运行如下命令。
```bash
pip install -r requirements.txt \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
```

#### 6.3.3 模型下载
```bash
# 从aliendao.cn下载gemma-2b模型文件
wget https://aliendao.cn/model_download.py
python model_download.py -e --repo_id alpindale/gemma-2b \
--token YP8XKHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/alpindale/gemma-2b目录下
```

#### 6.3.4 微调程序开发
微调过程使用Transformers库的SFTTrainer方法，采用PEFT方法的LoRA策略。语料采用Abirate/english_quotes数据集。微调程序名为gemma-train.py。源代码如下。
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, \
    BitsAndBytesConfig, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset
from peft import LoRaConfig

def load_Model():
    model_path = "./dataroot/models/alpindale/gemma-2b"
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, quantization_config=bnb_config, device_map={"": 0})
    return model, tokenizer

def load_data(tokenizer):
    dataset_path = "./dataroot/datasets/Abirate/english_quotes"
    data = load_dataset(dataset_path)
    data = data.map(lambda samples: tokenizer(samples["quote"]), \
                    batched=True)
    return data

def generate(text):
    device = "cuda:0"
    inputs = tokenizer(text, return tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=20)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

def formatting_func(example):
    text = f"Quote: {example['quote'][0]}\nAuthor: \
{example['author'][0]}"
    return [text]

def train(model, data):
    lora_config = LoRaConfig(
        r=8,
        target_modules=["q_proj", "o_proj", "k_proj",
                        "v_proj", "gate_proj", "up_proj", "down_proj"],
        task_type="CAUSAL_LM",
    )
    trainer = SFTTrainer(
        model=model,
        train_dataset=data["train"],
        args=TrainingArguments(
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=2,
            max_steps=10,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=1,
            output_dir="outputs",
            optim="paged_adamw_8bit"
        ),
        peft_config=lora_config,
        formatting_func=formatting_func,
    )
    trainer.train()

if __name__ == "__main__":
    print("torch cuda:", torch.cuda.is_available())
    model, tokenizer = load_Model()
    data = load_data(tokenizer)
    print(generate("Quote: Imagination is more"))
    train(model, data)
    print(generate("Quote: Imagination is more"))
```

对其中的主要函数进行说明，如表6-4所示。

**表6-4 Gemma-2B微调程序的主要函数说明**

|函数名|中文名|用途|
| ---- | ---- | ---- |
|load_Model|装载Gemma-2B模型|在本例中，采用load_in_4bit量化方式装载模型|
|load_data|装载数据集|载入Abirate/english_quotes数据集|
|generate|推理|在本例中，在微调前和微调后用相同的提示词来对比模型微调效果|
|train|微调过程|采用SFTTrainer方法微调模型|

#### 6.3.5 语料文件下载
使用以下命令下载数据集Abirate/english_quotes。
```bash
python model_download.py --repo_type dataset --repo_id \
Abirate/english_quotes
```
Abirate/english_quotes数据集的格式为jsonl，即每行数据都是一个JSON格式的对象，具体示例如下。
```json
[{"quote":"Be yourself; everyone else is already taken.","author":"Oscar Wilde","tags":["be-yourself","gilbert-perreira","honesty","inspirational","misattributed-oscar-wilde","quote-investigator"]},
{"quote":"I'm selfish, impatient and a little bit insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.","author":"Marilyn Monroe","tags":["best","life","love","mistakes","out-of-control","truth","worst"]}]
```
每行JSON数据都是一条完整的记录，格式如下。
```json
{
    "quote":"Be yourself;everyone else is already taken.",
    "author":"Oscar Wilde",
    "tags":[
        "be-yourself",
        "gilbert-perreira",
        "honesty",
        "inspirational",
        "misattributed-oscar-wilde",
        "quote-investigator"
    ]
}
```

#### 6.3.6 微调与测试过程

使用“python gemma-train.py”命令进行微调，由于数据集只有632KB，模型也是用量化模式装载的，所以微调过程很快，只需要几分钟即可完成。微调过程见图6-4。从结果来看，微调前补全的名言作者是“- Albert Einstein”，而由于新数据集的引入，微调后显示名言作者为“Author:Albert Einstein” 。

**图6-4 Gemma-2B微调与测试**（图中展示了微调过程中的命令行输出信息 ） 


![image](https://github.com/user-attachments/assets/3bb171a3-0c03-40c3-8464-7f485ea1b550)
