### 第5章 大语言模型安装
ChatGPT之所以如此流行，一个原因是技术水平飞速发展，GPT-3.5拥有强大的自然语言处理能力，能够生成准确且流畅的文本回复；另一个原因是采用了Chat方式，让用户感觉在与一个真实的人进行交流，这种互动方式吸引了大量用户的关注和使用。此外，开放的API帮助开发者将这一先进技术集成到自己的应用中，从而推动了ChatGPT的广泛应用和普及。 

随着ChatGPT的热潮席卷全球，开放的大语言模型如雨后春笋般迅速涌现，较少的参数量、不错的生成效果、算力要求不高等优势使得在本地部署大语言模型成为可能。一个70亿（7B）参数规模的大语言模型，可以装入16GB内存的GPU执行推理任务，基本可以应对日常的写作、编程等方面的需求。 

本章选取了ChatGLM3-6B、Qwen-VL-Chat-Int4、LLama-2-7b-chat、Gemma-2B和Whisper-large-v3模型进行部署。这五个比较典型的大模型分别代表了国产文本类、国产图像类、开源标杆、最新模型和语音识别类。为了更加贴近生产实际，部署完成后验证的方式也分为两种，有的使用Python编程，有的使用开源UI工具。虽然五款模型都采用了Hugging Face格式和Transformer架构，其安装过程较为类似，但又有所区别，而大语言模型应用的实践性非常强，所以本章给出了详细的过程描述，力求使读者掌握各种常见模型的安装部署和简单推理应用的相关知识。 

#### 5.1 ChatGLM安装
##### 5.1.1 ChatGLM3模型介绍
ChatGLM3是智谱AI和清华大学KEG实验室联合发布的对话预训练模型。ChatGLM3-6B是ChatGLM3系列中的开源模型，基础模型ChatGLM3-6B-Base采用了多样的训练数据、充分的训练步数和合理的训练策略，在语义、数学、推理、代码、知识等方面表现良好。ChatGLM3-6B采用了全新设计的Prompt格式，除正常的多轮对话外，同时原生支持工具调用、代码执行和Agent任务等复杂场景。ChatGLM3-6B、ChatGLM3-6B-Base和长文本对话模型ChatGLM3-6B-32K的参数权重已对学术研究领域完全开放，并允许用户在填写问卷登记后进行免费的商业应用。 

本节部署的模型是ChatGLM3-6B。在笔者对众多同等规模模型的使用实践中，ChatGLM3-6B的效果是最好的，开发活跃度也很高，该系列模型最近发布了新版本GLM-4，发展非常快。 


##### 5.1.2 ChatGLM3-6B安装

（1）下载源码

ChatGLM3一直在发展中，为了保证本项目调试的版本稳定性，我们会选取一个提交点的版本进行描述。这样不管ChatGLM3的源码如何变更，这个提交点的版本下的源码是保持不变的。Git代码库回退到某个提交点的命令是“git checkout commit_id”。 

```bash
git clone https://github.com/THUDM/ChatGLM3
cd ChatGLM3
git checkout c814a72
```
这样就将已下载到本地的ChatGLM3的所有文件都还原到了2024年3月8日、commit id为c814a72的版本。 


（2）创建Python虚拟环境


```bash

# 建立Python3.10虚拟环境并激活
conda create -n ChatGLM3 python=3.10 -y
conda activate ChatGLM3
```

（3）安装依赖库

Python的第三方库使用pip工具安装。由于库与库之间的版本有相互依赖关系，规范的做法是将项目所用到的所有第三方库的名字、版本号写入一个名为requirements.txt的文本文件，格式如下。 
```
protobuf>=4.25.3
transformers>=4.38.1
tokenizers>=0.15.0
……
```
安装库时，通过“pip install -r requirements.txt”命令进行批量安装，这里的“-r”是recursion（递归）的意思。 

通过pip从Python编程语言的官方软件仓库https://pypi.org/获取库文件，下载速度比较慢，可以采用阿里云、清华大学、中科大研究的镜像加速技术。将镜像路径以“-i”参数进行指定，再加到pip命令的尾部。为了确认安全性，还需要增加一个“--trusted-host”参数指向镜像所在的网站域名。本中，使用中科大发布的pypi镜像安装依赖库，命令如下。 
```bash
pip install -r requirements.txt \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
```
需要注意的是，这个命令是针对Linux的。如果在Windows上执行，则要去掉反斜杠“\”和换行符后将代码保持在一行上才能正常运行。 

（4）校验PyTorch

PyTorch是大语言模型最重要的库之一，对上为Transformers库提供计算功能，对下连接CUDA，因为PyTorch安装时与CUDA的版本关系密切。并且，在requirements.txt中往往只指定了最低版本要求，如“torch>=2.1.0”，导致pip执行时可能会安装PyTorch的最新版本。这样在安装过程中表现是正常的，但运行时会报错，因为PyTorch使用中出错的可能性比其他组件大得多。 

在ChatGLM3的Conda虚拟环境中，运行以下命令进行校验。 
```python
python -c "import torch; print(torch.cuda.is_available())"
```
如果返回“True”则说明校验通过，如果返回“False”或报错，则可参见前面讲述的方法，适当降低PyTorch的版本。 

（5）下载模型

huggingface.co是大语言模型的托管网站，从中下载模型时可以使用国内的镜像加速。主要的镜像包括阿里的魔搭社区（https://modelscope.cn）和笔者开发的异型岛（https://aliendao.cn），本书示例的模型都是从异型岛网站下载的。 

首先从aliendao.cn的首页下载model_download.py。对此，在Linux上用wget命令，在Windows上通过浏览器下载。然后用此脚本下载指定model_id的模型。 
```bash
# 模型脚本从aliendao.cn首页下载
# 链接为https://aliendao.cn/model_download.py
# Linux下使用Wget命令下载，Windows下直接在浏览器打开链接下载
wget https://aliendao.cn/model_download.py
# 从aliendao.cn下载chatglm3-6b模型文件
python model_download.py --e --repo_id THUDM/chatglm3-6b \
--token YPY8KHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/THUDM/chatglm3-6b目录下
```

##### 5.1.3 编程验证

为了了解Transformers库调用大模型的工作过程，我们编写一个Python程序，即chatglm3-inference.py。该程序实现的功能为：用户输入问题，模型给出回答，按下回车键退出。文件内容如下。 
```python
from transformers import AutoTokenizer, AutoModel

model_path = "./dataroot/models/THUDM/chatglm3-6b"
# 装载tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path, trust_remote_code=True)
# 装载模型
model = AutoModel.from_pretrained(
    model_path, trust_remote_code=True)
# 将模型载入GPU
model = model.to("cuda")
# 将模型设置成评估模式
model = model.eval()
# 推理
while True:
    prompt = input("请输入问题，回车退出：")
    if prompt == "":
        break
    response, history = model.chat(tokenizer, prompt, history=[])
    print(response)
```
运行“python chatglm3-inference.py”命令，测试模型的推理效果，见图5-1。图上加框标注的部分是用“watch -n 1 -d nvidia-smi”命令监控的GPU内存使用率指标。 

![image](https://github.com/user-attachments/assets/ba018214-00a0-46af-b6ea-9890e154f4d5)


按照程序中的写法，对推理卡的最低内存要求是16GB，因为将模型完整载入GPU，需要13GB左右的GPU内存，如果内存不够，则会出现OOM（Out Of Memory，内存溢出）错误，显示的错误如下。 
```
torch.cuda.OutOfMemoryError: CUDA out of memory.
```
如果推理卡的内存小于16GB，则模型可以改为量化模式载入，甚至在没有GPU的情况下可用CPU运行（当然速度非常慢）。图5-1显示推理卡只有8GB内存，所以采用了4bit量化装载的方法，只占用了不到5GB的内存，具体操作是修改以下代码。 
```python
model = model.to("cuda")
```
修改情况见表5-1。 
**表5-1 ChatGLM3-6B量化装载参数**

| 装载代码 | 模式 | GPU内存占用 |
| ---- | ---- | ---- |
| model = model.quantize(4).to("cuda") | 4bit量化 | 5GB |
| model = model.quantize(8).to("cuda") | 8bit量化 | 9GB |

#### 5.2 Qwen-VL安装
##### 5.2.1 Qwen模型介绍
通义千问（Qwen）是阿里云推出的大模型，当前基础模型已经稳定训练了大规模、高质量且多样化的数据，覆盖多种语言（当前以中文和英文为主），拥有高达3万亿个token，并且利用SFT和RLHF技术实现对齐，从基础模型训练得到了对话模型Qwen-Chat。该对话模型具备聊天、文字创作、摘要总结、信息抽取、翻译等能力，同时具备一定的代码生成和简单数学推理的能力。在此基础上，对大模型对接外部系统等方面的功能做了优化，所以该模型当前具备较强的工具调用能力，还能提供最近备受关注的代码解释器（Code Interpreter）和智能代理（Agent）功能。 

除了基础模型Qwen和微调后的文本类模型Qwen-Chat外，通义千问还有大规模音频语言模型Qwen-Audio和大规模视觉语言模型Qwen-VL。Qwen涉及领域较多，在量化运行方面支持llama.cpp，是国产大模型领域中产品和生态链比较齐全的模型。本节介绍部署Qwen-VL的详细过程，实现图生文的效果。 

##### 5.2.2 Qwen-VL-Chat-Int4安装
Qwen-VL是一种视觉语言模型，相较文本类模型，视觉模型占用的算力资源更高，即使对Qwen-VL-Chat模型降低精度进行4bit量化装载，也需要12GB左右的GPU内存来运行。Qwen-VL-Chat-Int4运行时会用到AutoGPTQ库 ，由于AutoGPTQ需要编译安装，所以建议在Linux下实践。 
```bash
#（1）建立Python3.10虚拟环境并激活
conda create -n Qwen-VL-Chat python=3.10 -y
conda activate Qwen-VL-Chat

#（2）建立测试目录
mkdir Qwen-VL-Chat-Int4
cd Qwen-VL-Chat-Int4

#（3）下载模型
# 安装下载脚本model_download.py的依赖库
pip install transformers==4.32.0 \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
# 下载模型
wget https://aliendao.cn/model_download.py
python model_download.py --e --repo_id Qwen/Qwen-VL-Chat-Int4 \
--token YPY8KHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/TQwen/Qwen-VL-Chat-Int4目录下

#（4）安装基本依赖库
pip install -r./dataroot/models/Qwen/Qwen-VL-Chat-Int4/requirements.txt \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn

#（5）安装其他依赖库
pip install optimum gekko \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn

#（6）验证PyTorch
python -c "import torch; print(torch.cuda.is_available())"
# 如果返回“True”则说明校验通过；如果返回“False”或报错，则可参见前面讲述的方法来解决。

#（7）安装AutoGPTQ库
git clone https://github.com/JustinLin610/AutoGPTQ.git
cd AutoGPTQ
pip install -v \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
# 安装完成后回退到Qwen-VL-Chat-Int4目录
cd..
```

##### 5.2.3 编程验证
编写Python程序qwen-vl-inference.py。该程序可以实现用户向模型输入一张图片，模型给出对该图片的描述。该程序的内容如下。 
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 检验是否支持GPU
print(torch.__version__)
print(torch.cuda.is_available())
# 设置生成随机种子初始化网络
torch.manual_seed(1234)
# 模型路径
model_path = "./dataroot/models/Qwen/Qwen-VL-Chat-Int4/"
# 装载tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, \
trust_remote_code=True)
# 装载模型并设置评估模式
model = AutoModelForCausalLM.from_pretrained(
    model_path, device_map="cuda", trust_remote_code=True).eval()
# 输入图片与进行文本预处理
query = tokenizer.from_list_format([
    {'image': 'https://pic4.zhimg.com/80/v2-4e6e736dbfa729e7c61719fb3ee87047_720w.webp'},
    {'text': '这是什么'},
])
# 推理
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
```
输入的原图如图5-2所示。 

![image](https://github.com/user-attachments/assets/038cf80a-543c-4179-8519-e52f6b78fe8b)


运行“python qwen-vl-inference.py”命令测试模型的推理效果，见图5-3。 

![image](https://github.com/user-attachments/assets/8a8fefb6-58b8-4561-a2dd-b0de4ceeb0c0)


#### 5.3 LLaMA2安装
##### 5.3.1 LLaMA2模型介绍
LLaMA是在ChatGPT迅速流行后，由Meta发布的开源大语言模型，基于Transformer架构实现。ChatGPT后台的模型GPT-3.5是闭源的，当时与之能力相当的大语言模型少之又少，而Meta AI开启了大语言模型开源的先河。Meta AI公布了模型权重和训练方法，提供了免费的许可证，允许个人和组织在商业项目中免费使用或修改LLaMA模型。这一举措为大模型的创新和发展提供了更大的空间，使更多人能够利用先进的自然语言处理技术，促进形成了以LLaMA为基础的生态，使LLaMA成了很多模型项目的首选。 

2023年7月，性能更强的Meta LLaMA2发布，它采用了分组的查询注意力机制，提供了70亿、130亿和700亿参数规模的基础模型，能够在各种自然语言处理任务上取得令人印象深刻的成绩。首先，对于文本生成、情感分析、语义理解和机器翻译方面的任务，LLaMA2都能够胜任，并且在许多基准测试中表现良好。其次，LLaMA2还具有较强的灵活性和可定制性，用户可以根据特定的需求和应用场景对模型进行微调和定制，从而获得比较好的效果。这些优势使得LLaMA2成为各行业应用的理想选择，适用于学术研究、商业和各领域应用的理想选择，适用于学术研究、商业应用或者个人项目。 

美中不足的是，由于LLaMA2训练语料中仅有0.13%的中文语料 （各种语言占比详细情况见图5-4），所以其基础模型的中文表现较差，需要经过二次训练才能取得较好的中文输出效果。但从技术方面讲，LLaMA2的架构非常值得研究。 

![image](https://github.com/user-attachments/assets/eb8ac460-2c56-4b40-a9f8-bf48eb5c4c60)


**图5-4 LLaMA2训练语料中各种语言占比**

| 语言 | 占比 | 语言 | 占比 |
| ---- | ---- | ---- | ---- |
| en | 89.70% | uk | 0.07% |
| 未知 | 8.38% | ko | 0.06% |
| de | 0.17% | ca | 0.04% |
| fr | 0.16% | sr | 0.04% |
| sv | 0.15% | id | 0.03% |
| zh | 0.13% | cs | 0.03% |
| es | 0.13% | fi | 0.03% |
| ru | 0.13% | hu | 0.03% |
| nl | 0.12% | no | 0.03% |
| it | 0.11% | ro | 0.03% |
| ja | 0.10% | bg | 0.03% |
| pl | 0.09% | da | 0.02% |
| pt



