


![image](https://github.com/user-attachments/assets/826fe1bc-8a6b-40a3-bd6f-bb9c4b32d2e3)

### 图6-1 ChatGLM3-6B P-Tuning v2微调（续）

此时就可以中断微调了。微调过程中的各种运行指标见表6-1。

**表6-1 ChatGLM3-6B微调情况**

|类别|参数|参数值|
| ---- | ---- | ---- |
|GPU|GPU内存占用|14.8GB|
| |GPU利用率|100%|
| |GPU温度|79℃（正常）|
|微调过程|总步数|3000|
| |已进行步数|17|
| |剩余时间|3h|
| |目前loss值|4.8|

在大语言模型进行微调时，判定微调效果不是靠步数，而是要参考训练过程中的loss值。一般来说，当loss值趋于稳定且不再显著下降时，即可以认为模型已经收敛。判断模型微调效果是否趋于稳定，有一个经验值——0.3。当loss值低于这个值时，说明微调模型基本可用。当然，实际操作中模型不同、参数量不同、任务不同、数据集不同、微调方法不同，不能一味用这个固定的loss值来判断，最好的做法是根据实际情况来测试和评估模型的性能。所以，当进行了3000步微调后，若loss值依然很大或评估发现效果不佳，则还要载入3000步的检查点继续微调，以达到更好的效果。

#### 6.1.6 微调模型测试
微调过程中，按照默认参数，每500步会产生一个检查点并将其保存到output目录下，如output/checkpoint-2500。全部训练完后，检查点为output/checkpoint-3000。在训练过程中，如果还有空闲算力，则可以载入检查点进行测试，如果暂无空闲算力，则等3000步运行完退出微调程序后再测试。测试的命令如下。
```bash
python finetune_demo/inference_hf.py checkpoint_path --prompt 问题
```
如：
```bash
python finetune_demo/inference_hf.py output/checkpoint-3000 \
--prompt 高血压要注意哪些方面？
```
运行结果如图6-2所示。

![image](https://github.com/user-attachments/assets/16a017fe-7ab3-486c-af49-ef94c885cc57)


**图6-2 ChatGLM3-6B微调后推理**（图中展示了运行命令及模型输出关于高血压注意事项的内容 ）

对于微调前原始模型的效果，可用以下命令查看。这样就可以对比微调前和微调后的模型生成效果。
```bash
python finetune_demo/inference_hf.py \
./dataroot/models/THUDM/chatglm3-6b \
--prompt 高血压要注意哪些方面？
```

### 6.2 LLaMA2微调
本节参考LLaMA2官方提供的微调方法，使用PEFT技术，加入中文语料对基础模型Llama-2-7b-chat-hf进行微调，并将微调模型与原始合并，编写推理代码。通过学习LLaMA2微调的全流程，掌握基于Transformer架构的大语言模型的基本微调原理和操作过程。

#### 6.2.1 微调方法介绍
对于LLaMA2，采用Meta官方提供的llama-recipes程序进行微调。具体有两种微调手段可供选择，分别为参数高效模型微调（Parameter Efficient Model Fine-Tuning，PEFT）和全/部分参数微调（Full/Partial Parameter Fine-Tuning）。PEFT由Hugging Face开发，支持LoRA等微调策略，对算力要求较低，所以本节的微调实例采用了PEFT方法。

1. **参数高效模型微调**

参数高效模型微调只需占用较少的GPU内存资源，可以在一个消费级GPU（如RTX3090）上微调。经过实践，Llama-2-7b或Llama-2-7b-chat的半精度模型，可以在16GB的GPU（如P100、T4）上微调。参数高效模型微调的原理是保持整个模型参数冻结，只在模型上添加微小的可学习参数或层，这样就只训练一小部分参数。此类微调中常常采用的策略有LoRA、LLaMA适配器（LLaMA Adapter）和前缀微调（Prefix-Tuning）。其中LoRA在1.4.4节已经介绍过，是一种通过低秩矩阵分解来实现参数高效微调的方法，可显著减少微调过程中需要更新的参数数量，从而提高效率。LLaMA Adapter是一种专门用于LLaMA模型的参数高效微调方法，其核心思想是在模型的不同层之间插入适配器模块，通过微调这些适配器来实现任务适应，而不是直接修改模型的原始参数。Prefix Tuning是一种在输入序列前添加可微调的前缀向量来实现参数高效微调的方法，主要是利用前缀向量在输入序列中的影响，通过微调这些前缀向量来适应新任务，而保持原始模型参数不变。

该微调方法的优势主要有以下几个方面。

- **低微调成本**：这些方法只训练一小部分额外的参数，而不是完整的模型，这使得在消费级GPU上调整这些参数成为可能。
- **低部署成本**：微调的只有一小部分参数，只占用MB级空间而不是GB级。这样在装载模型时，除装载原始的LLaMA2预训练模型外，只需要再额外加上几个MB的参数就可完成微调后模型的部署工作，实现了以预训练模型为主干，通过微调参数适应不同任务，从而进行联合推理的目的。 
- **减少灾难性遗忘问题**：由于原始模型的整体参数是锁定的，大模型原本具备的知识不会因为微调而丢失。 

2. **全/部分参数微调**

全参数微调有三种策略：保持预训练模型冻结，仅微调任务头，如分类器模型；保持预训练模型冻结，并在顶部添加一些完全连接的层；对所有图层进行微调。

全/部分参数微调的对象是原始模型的权重参数。如果GPU内存较小，那么一个GPU往往无法进行微调，因为没有足够的GPU内存来保存模型参数、梯度和优化器状态，而具体占用多少内存，与模型参数数量、精度有关。在这种情况下，需要引入完全分片数据并行（Fully Shared Data Parallel，FSDP）的分布式训练方案，使得将模型分布到“一机多卡”“多机多卡”等环境下进行微调成为可能。FSDP不仅对数据进行分片，还对参数、梯度和优化器状态进行建模。这意味着每个GPU将只保留模型的一个分片，这将节省大量内存，使我们能够将更大的模型安装到多个GPU中。


#### 6.2.2 微调环境准备
- **代码准备**
```bash
git clone https://github.com/facebookresearch/llama-recipes
cd llama-recipes
git checkout 95418fc
```
- **Python3.10环境创建**
```bash
conda create -n llama-recipes python=3.10 -y
conda activate llama-recipes
```
- **更新setuptools**
```bash
pip install -U pip setuptools
```
- **安装基本依赖库**
```bash
pip install -r requirements.txt \
-i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
```
- **验证PyTorch是否安装成功**
```bash
python -c "import torch; print(torch.cuda.is_available())"
```
- **使用源码安装llama-recipes模块**
```bash
pip install -e. -i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
```
- **PEFT为v0.10.0或更高版本时，进行微调会报以下错误**
```
cannot import name prepare_model_for_int8_training from peft
```
- **需要将PEFT版本降低到v0.9.0**
```bash
pip install peft==0.9.0 -i https://pypi.mirrors.ustc.edu.cn/simple \
--trusted-host=pypi.mirrors.ustc.edu.cn
```

#### 6.2.3 语料准备

微调LLaMA2模型可采用alpaca_dataset数据集。alpaca_dataset是一个斯坦福开源数据集，包括用于特定领域模型微调Alpaca 的52000组指令数据。其文件内容是一个字典列表，每个字典包含以下字段：instruction（指令）、input（输入）和output（输出）。

- **instruction**：描述模型应执行的任务。52000组数据中，每一组数据要指定唯一的instruction，不能重复。

- **input**：任务的可选上下文或输入内容。例如，当instruction为“总结以下文章”时，输入内容就是文章本身。 

- **output**：由text - davinci - 003（ChatGPT-3.5背后的大语言模型）生成的答案。



alpaca_dataset中文语料很容易从网上搜索到，可以从https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json中下载。

打开此链接，点击“Download raw file”下载alpaca_dataset语料，并进行修改，或者将其他中文语料转换为alpaca_dataset格式。语料文件命名为alpaca_data.json，格式如下。
```json
[
    {
        "instruction": "你是谁？",
        "input": "",
        "output": "我是Gitclone，一个语言模型，由来自git-cloner的研究人员训练。"
    },
    {
        "instruction": "你能做什么呢？",
        "input": "",
        "output": "我可以和你聊天"
    },
    {
        "instruction": "你是谁啊？",
        "input": "",
        "output": "我叫Gitclone，我是git-cloner开发的语言模型。"
    }
]
```

#### 6.2.4 模型下载
本节微调的模型是Llama-2-7b-chat-hf，执行以下步骤下载模型。
```bash
# 从aliendao.cn下载Llama-2-7b-chat-hf模型文件
wget https://aliendao.cn/model_download.py
python model_download.py -e --repo_id NousResearch/Llama-2-7b-chat-hf \
--token YP8XKHDQ2NAHQ2SG
# 下载的文件在./dataroot/models/NousResearch/Llama-2-7b-chat-hf目录下
```

#### 6.2.5 微调过程

1. **微调语料准备**

语料文件命名为alpaca_data.json。微调时，需要将该文件复制到llama-recipes/src/llama_recipes/datasets目录下。

2. **微调参数说明**

微调过程使用recipes/finetuning/finetuning.py脚本。微调脚本用到的所有参数均在src/llama_recipes/configs/training.py文件中。需要特别注意的是，如果PyTorch的版本稍旧，那么微调脚本用到的torch.distributed._tensor.device_mesh类可能会存在问题。因为PyTorch的某个版本中缺少init_device_mesh方法。如果微调时报错信息为“import init_device_mesh”，则要注释掉llama-recipes/src/llama_recipes/utils/fsdp_utils.py中的第三行代码。

```python
# from torch.distributed._tensor.device_mesh import init_device_mesh
```

根据推理卡的内存和算力情况，确定微调的参数，并不断对其进行调整，以满足“GPU内存基本占满，GPU使用率基本撑满”的最经济运行原则，最大程度地利用现有算力条件。表6-2为对于“单机单卡”的16G内存GPU的一些模型微调建议参数。

**表6-2 Llama-2-7b-chat-hf微调建议参数**

|参数名|说明|建议值|
| ---- | ---- | ---- |
|model_name|基础模型文件路径|/dataroot/models/NousResearch/Llama-2-7b-chat-hf|
|output_dir|微调后输出路径| |
|dataset|语料类型，可选samsum_dataset、grammar_dataset、alpaca_dataset、custom_dataset|Alpaca_dataset|
|use_peft|使用PEFT微调|True|
|peft_method|PEFT方法，可选None、lora、llama_adapter、prefix|lora|
|batch_size_training|训练批量大小，可根据GPU的内存情况调整，在内存不溢出的情况下尽量将该值设置大一些|4|
|num_epochs|训练轮次|3|
|context_length|上下文长度，默认为4096，可根据GPU的内存情况调整，在内存不溢出的情况下尽量将该值设置大一些|1024|
|quantization|8bit量化装载模型|True|


3. **微调过程**

使用以下脚本微调。因为微调的时间较长，为了防止SSH终端工具异常断开导致微调过程中断，建议用nohup命令在后台运行微调进程。这样即使SSH终端断开，也不影响微调过程。运行日志写入了train.log，可用“tail -f train.log”命令查看。

```bash
CUDA_VISIBLE_DEVICES=0 nohup torchrun \
--nnodes 1 --nproc_per_node 1 recipes/finetuning/finetuning.py \
--model_name./dataroot/models/NousResearch/Llama-2-7b-chat-hf \
--output_dir output/PEFT/model \
--dataset alpaca_dataset \
--use_peft \
--peft_method lora \
--batch_size_training 4 \
--num_epochs 3 \
--context_length 1024 \
--quantization > train.log 2>&1 &
# 日志查看
tail -f train.log
```
微调过程见图6-3。过程中的各种运行指标见表6-3。

**图6-3 Llama-2-7b-chat-hf微调**（图中展示了GPU相关信息及运行进程的GPU内存使用情况 ） 


![image](https://github.com/user-attachments/assets/a4fd7780-eda4-4391-a098-5f3ada7eed12)
