通过一个单独的注意力头进行处理。这些注意力计算结果最终被融合在一起，形成注意力分数，这种机制被称为多头注意力，能使得Transformer更好地捕捉每个单词之间的多重关系和细微差别。自注意力关注序列内每个位置对其他所有位置的重要性，而多头注意力则通过在多个子空间中并行计算注意力，使模型能够同时捕获和整合不同方面的上下文信息，从而增强了对复杂数据内在结构的分析能力。

### 1.3 应用开发技术
#### 1.3.1 Python
Python是一种高级编程语言，由Guido van Rossum在1989年首次发布。Python代码可读性高、便于维护、语法简洁，且Python是面向对象编程，被广泛用于数据科学、机器学习、人工智能、Web开发、自动化脚本和游戏开发等领域。

随着人工智能和自然语言处理领域的不断发展，Python在GPT应用开发中扮演着越来越重要的角色，并且得到了持续的更新。PyTorch、TensorFlow、NumPy和Transformers这些库提供了构建和训练GPT模型所需的工具与资源，开发人员可以利用Python的简洁语法和强大的库来快速实现各种GPT应用。在本书中，后台应用都采用了Python来进行编写，仅使用100行以内的代码就可以实现复杂、功能强大的应用。


下面介绍一下书中涉及的两个最主要的库：PyTorch和Transformers。

1. **PyTorch**

PyTorch是Torch的Python版本，Torch是一个开源的机器学习库，最初由Facebook（Meta）开发，提供了许多用于构建深度学习模型的工具和算法。Torch使用Lua编程语言，提供了一个灵活的张量库，可以在GPU上高效地运行。PyTorch提供了与Torch类似的功能，它使用Python作为主要编程语言，提供了许多方便的工具和接口，使得构建和训练深度学习模型更加简单、直观。

2. **Transformers**

Transformers库与Transformer有关。Transformer是大语言模型开发中使用的一种深度学习架构，而Transformers是Hugging Face开发的一个开源的自然语言处理库，支持许多预训练Transformer模型（如BERT、GPT、LLaMA），提供了方便的API和工具，使开发者使用这些预训练模型变得更加简单和高效。开发者可以轻松地加载、微调和使用这些模型，从而快速构建和训练自己的NLP模型应用。


#### 1.3.2 React.js
React.js是一种用于构建用户界面的JavaScript库，由Facebook（Meta）开发，使用组件化的方式来构建UI，使开发人员能够更加高效地构建复杂的Web应用程序。在本书提供的开发案例中，除个别应用以命令行模式运行或以Python构建Web应用外，其他的都需要开发客户端，本书选用React.js演示客户端开发，主要是考虑到了React.js的以下优势。
- **组件化UI**：React.js使用组件来构建UI，使开发人员能够更加清晰地组织代码，并且能够更好地维护代码。本书中经常使用的客户端Chat组件，就是使用了开源软件ChatUI 定制的。
- **动态性**：React.js支持动态渲染，能够实时地更新UI，使应用程序更加响应式。
- **热更新**：React.js支持热更新，能够实时地更新UI，这样修改源代码的结果会立刻反映到浏览器里，不需要重启服务，也不需要刷新网页，应用程序开发效率更高。 
- **Node.js支持**：React.js应用程序在开发过程中，需要Node.js环境的支持，而大语言模型提供了OpenAI的兼容推理接口，这样客户端就可以使用openai-node组件调用大模型服务。从客户端的角度来看，统一的接口消除了访问不同大模型方法的差异。

### 1.4 训练方法
本书所涉及的大语言模型大多是预训练模型。在相关语境中，训练（Training）和微调（Fine-Tuning）这两个概念的界限比较模糊，提及“训练”时往往指的是“微调”。但预训练（Pre-training）和微调则完全不同，预训练是指训练出一个已经掌握了各种语言特征规律及人类知识的模型，并且用参数来固定这些已经学到的知识。而微调是指调整大语言模型的参数以适应特定任务的过程，是通过在与任务相关的数据集上训练模型来完成的。微调的基本原理是在训练开始时对预训练模型中的权重进行初始化，然后在训练过程中逐步调整这些权重，以适应新的下游任务，如对话、翻译、文本分类、写摘要等。举例来说，ChatGPT使用的模型是在基础模型davinc上微调形成的，GLM基础模型在微调后生成对话模型ChatGLM。

微调过程虽然复杂，但相对于训练一个全新模型而言，它只涉及对参数进行细微调整。此外，微调所使用的数据集通常相对较小。目前主流的微调方法有FFT、RLHF、P-Tuning和LoRA等。与训练一个预训练模型需要海量的语料不同，微调的数据量较小，一般由52000个任务数据组成。

#### 1.4.1 FFT
FFT（Full Fine Tuning，全量微调）是一种对整个预训练模型的所有参数进行微调的方法，预训练模型的所有层和参数都可能会被更新和优化，以适应新加入的语料。这种微调方法通常适用于微调任务和预训练模型之间存在很大差异的情况。比如，预训练模型是一个基础模型，只包含常规的知识和语法结构，而微调的目标是让其能用于金融服务。全微调需要较多的算力资源和时间，但可以获得比较好的效果。

https://chatui.io/，由阿里巴巴开发。

#### 1.4.2 RLHF
RLHF是Reinforcement Learning from Human Feedback（人类反馈强化学习）的缩写，是指基于人类反馈对大语言模型进行强化学习，让大模型生成的结果更符合人类的期望。RLHF是ChatGPT模型训练的手段之一。RLHF训练流程分为以下三步。
- **监督微调（Supervised Fine-Tuning，SFT）**：使用人工精选的问题与答案，对预训练的语言模型进行微调，形成多个预选模型。
- **奖励模型微调（Reward Model Fine-Tuning，RW）**：用多个模型（初始模型、微调模型）给出问题的多个回答，然后人工对这些回答按一些标准（可读性、无害、正确性）进行排序，训练一个奖励模型/偏好模型来进行打分。 
- **RLHF训练**：利用近端策略优化（Proximal Policy Optimization，PPO）算法，根据RW模型的奖励反馈进一步微调SFT模型。

实践RLHF微调并不需要很高的算力，可以在消费级的推理卡上进行，Deepspeed-chat 给出了完整的RLHF三阶段微调方法和示例。

https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat。

#### 1.4.3 P-Tuning
P-Tuning是ChatGLM模型专用的微调方式，是在微调方法的基础上进行改进而形成的一种参数高效的微调方法。P-Tuning过程中只有少量参数参与微调，训练计算复杂度相对较小，其原理是将人工提示词模板转换成可学习的连续向量，通过学习下游任务来优化这些向量，从而自动生成效果良好的自动提示模板。P-Tuning v2对该方法进行了适当优化，使其在各种规模的模型中普遍有效。优化后的提示调整策略在中小型模型和序列标注任务上实现了与传统微调相当的性能。

#### 1.4.4 LoRA
LoRA （Low-Rank Adaptation of large language model，大语言模型的低秩适配）的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的基础上向模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数的数量较少，这样不仅微调的成本显著下降，还能获得和全模型参数参与微调类似的效果。在实践过程中，经过LoRA微调的模型，对新加入的语料知识进行学习的效果显著，是一种低算力、低成本但效果较好的微调方法。

https://arxiv.org/pdf/2106.09685.pdf。

### 1.5 常见现象
大语言模型应用领域经常会出现一些“非专业术语”，描述一些现象，在这里我们做一下详细说明。

#### 1.5.1 幻觉
大模型幻觉（Hallucination）指大模型在面对某些输入时，产生不准确、不完整或误导性的输出，即“一本正经地说胡话”，比如输出不准确的回答、不存在的参考文献、对最近新闻的不合理评论等。幻觉的形成原因有很多，如模型参数太少、语料时效性差、语料数量或训练不足。总之，因为大语言模型的强大能力，人们误认为它能回答一切问题，实际上这是不可能的。模型为了避免自身知识储备不足导致“拒绝回答”造成用户不信任，会生成语义正确但逻辑错误的答案。这种情况被称为“幻觉”，但该现象名称的诗意实际上无法掩盖问题的严重性。

#### 1.5.2 灾难性遗忘
灾难性遗忘（Catastrophic Forgetting）通常发生在大语言模型进行参数微调后，学习新知识的过程会迅速破坏之前获得的信息。简单来说，微调过程中大语言模型的参数发生变化，模型将原来学到的知识“忘记”，输出效果还不如原始模型，原因多是语料质量差、训练不够、噪声干扰、过度拟合微调数据集等。

#### 1.5.3 涌现
涌现是指模型未经直接训练，突然而不可预测地展现出的能力。例如，在用大量自然语言文本训练后突然能够回答并未见过的问题，总结段落或文章的内容，解答数学题并达到较高的准确率。这个能力是在模型参数达到一定规模或训练到一定阶段突然出现的非线性增长，见图1 - 8。 

![image](https://github.com/user-attachments/assets/263af410-c2b2-45fe-bcca-f5eb75a92b4d)



**图1 - 8 涌现示意**

横轴：参数规模（1亿、3.5亿、10亿、30亿、70亿、130亿 ）

纵轴：模型能力


但对于是否真的存在这种难以解释的涌现情况，业界目前有两种观点。

1. **确实存在涌现**：随着参数量和训练数据增加，规模较大的模型确实会突然具备规模较小的模型所不具备的能力，而且不能从规模较小的模型的表现推导出在什么规模条件下新能力会出现。 

2. **不存在涌现**：这是不完善的评估方法造成的错觉，使用非线性或不连续的评估标准会造成涌现的假象，而通过线性或连续的评估标准可以相当好地预测大语言模型的能力。


#### 1.5.4 价值对齐
随着大模型开始从事语言理解和内容生成工作，人们需要面对一个非常有挑战性的问题：如何让大模型的能力和行为跟人类的价值观、真实意图和伦理原则保持一致？这个问题被称为“价值对齐”（Value Alignment）。价值对齐是AI安全的一个核心议题，一个没有价值对齐的大语言模型，可能输出含有种族歧视、性别歧视、违法或不良倾向的内容。对此，人类反馈强化学习（RLHF）被证明是一个有效解决价值对齐问题的方法，它通过少量的人类反馈数据就可能实现比较好的对齐效果。模型的对齐步骤必不可少，但如果开发并运营一个基于大语言模型的产品，只进行模型对齐是不够的，还要在应用端做大量的信息处理工作，拦截明显不符合人类价值观的问题和答案。 
