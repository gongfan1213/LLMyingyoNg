### 第7章 Chapter 7 大语言模型量化
大语言模型是将语言问题转换为数学计算的一种人工智能技术。由于大量采用浮点运算，大语言模型一般都需要GPU算力的支持，即使是参数规模较小的6B或7B模型，也需要至少16GB内存的推理卡才能正常运行。对于不具备这样的算力条件，又想实践大语言模型应用的情况，也有相应的解决方案，即通过模型量化（Quantization）的方法来实现。

本章介绍了在笔记本计算机等低算力条件下部署大语言模型的方法，也就是在Windows操作系统上，使用llama.cpp、gemma.cpp等量化方法，实现AI问答系统，从而在低算力条件下运行完整的大语言模型应用。

#### 7.1 量化介绍

大语言模型通常有数十亿甚至数百亿个参数，运行时会占用大量的GPU内存和计算资源。当既想使模型在有限的算力条件下运行，又不想使模型的效果降低太多时，模型量化方法则成为一种可行的选择。

模型量化是指将模型中的权重参数或激活值从32位浮点型的高精度转换为4bit或8bit的低精度整数的过程，得到的结果是精度降低、模型变小、计算的复杂度降低，使模型在CPU等低算力条件下也能正常推理。在量化的实践过程中，除降低精度外，还要将原来运行在Python环境中的程序使用C++语言重新编写，这样才能够基本保持模型的能力，又保证运行性能。需要注意的是，量化方法通常只用于推理过程，而不能应用于训练阶段。



#### 7.2 llama.cpp量化过程

llama.cpp是一种基于LLaMA架构的大模型量化方法，是由开源开发者Georgi Gerganov使用纯C/C++语言开发的专用于推理的版本。其目标在于让大模型可以在除GPU之外更广泛的硬件上运行，甚至可以运行在Android手机上。

llama.cpp基于机器学习模型张量库GGML构建。GGML也是由Georgi Gerganov使用C/C++实现的，经过处理的大语言模型GGML版本可以在CPU上实现高性能的运行。

为了对Hugging face格式大语言模型进行量化，需要先把模型权重文件转换成GGUF格式。GGUF的全称是GPT-Generated Unified Format，是由Georgi Gerganov定义的一种大模型文件格式。它是一种二进制文件规范，采用紧凑的二进制编码格式、优化的数据结构、内存映射等技术，让大模型更加高效地进行存储和交换。原始的大模型预训练模型文件经过转换后成为GGUF格式，可以更快地被装载到内存使用，消耗更少的资源。

本节使用llama.cpp对Qwen-1.8B大模型进行量化，使其脱离原有的GPU环境，在Windows和CPU上运行。


##### 7.2.1 编译

llama.cpp采用C/C++实现。为了保证它在各种操作系统下的兼容性，使用llama.cpp前需要对其源码进行编译，生成main.exe、server.exe、quantize.exe等工具。为了简化编译过程，避免下载Visual Studio等重型工具，本节采用W64devkit开发包编译llama.cpp。W64devkit是一个可移植的Windows C和C++开发包，用于在Windows x64系统上编译C和C++语言的应用程序。

1. **安装w64devkit**

与Linux下预装的GCC编译环境不同，在Windows系统上编译C/C++程序时往往需要安装规模较大的Visual Studio环境。该安装过程相对复杂，所以为了简化编译过程，本节应用了w64devkit.exe编译程序作为替代方案。w64devkit的下载链接如下。

https://github.com/skeeto/w64devkit/releases/download/v1.21.0/w64devkit-1.21.0.zip

下载后直接解压缩即可。

2. **获取llama.cpp源码**

在命令行下运行“git clone”命令，从github.com中拉取llama.cpp源码，命令如下。
```js
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
https://github.com/ggerganov/llama.cpp
https://github.com/ggerganov/ggml
git checkout 19885d2
```
3. **编译**
w64devkit可被看成一个编译虚拟环境，运行w64devkit.exe就进入了一个类似于Linux的虚拟环境中，可执行shell命令和编译命令。对llama.cpp的编译过程极为简单，只用一行make命令即可编译生成llama.cpp的目标.exe文件。下面运行w64devkit.exe文件，切换到llama.cpp目录下，运行make命令编译。
```js
cd llama.cpp
make
```
编译的结果是产生了一系列llama.cpp可执行文件。其中，本节用到的有三个文件：main.exe、quantize.exe和server.exe。需要注意的是，Windows的杀毒软件可能会把server.exe当成威胁删除，必要时要从威胁列表中将其恢复。

##### 7.2.2 模型GGUF格式转换

本节量化的原始模型是Qwen-1.8B，其中模型文件是Hugging Face的格式，文件名为model-00001-of-00002.safetensors和model-00002-of-00002.safetensors。为了便于后续进行INT4量化，先用convert-hf-to-gguf.py脚本将safetensors格式的模型文件转换成GGUF格式。这个转化过程需要在Python虚拟环境下完成。

（1）**建立Python虚拟环境**

```
conda create -n llama.cpp python=3.10 -y

conda activate llama.cpp

```
（2）**安装依赖库**
```
# 基本依赖库
pip install -r requirements.txt -i https://pypi.mirrors.ustc.edu.cn/simple --trusted-host=pypi.mirrors.ustc.edu.cn
# tiktoken库
pip install tiktoken -i https://pypi.mirrors.ustc.edu.cn/simple --trusted-host=pypi.mirrors.ustc.edu.cn
```
##### 7.2.3 模型下载
从aliendao.cn下载Qwen-1.8B模型文件，代码如下。
```
# 从aliendao.cn下载Qwen-1.8B模型文件
wget https://aliendao.cn/model_download.py
python model_download.py -e --repo_id Qwen/Qwen-1_8B --token YPY8KHDQ2NAHQ2SG
# 下载后的文件在dataroot/models/Qwen/Qwen-1_8B目录下
```
##### 7.2.4 量化过程
量化模型分为两步，首先用convert-hf-to-gguf.py脚本将原始模型的精度降低一半，然后用quantize.exe命令将半精度模型量化为GGML格式，这样将大大降低模型对算力资源的要求。由下面代码可见，量化的结果是Qwen-1_8B/ggml-model-q5_k_m.gguf q5_k_m。
```
python convert-hf-to-gguf.py dataroot/models/Qwen/Qwen-1_8B
quantize.exe dataroot/models/Qwen/Qwen-1_8B/ggml-model-f16.gguf dataroot/models/Qwen/Qwen-1_8B/ggml-model-q5_k_m.gguf q5_k_m
```
##### 7.2.5 量化模型测试
经过量化的模型，就可以用main.exe装载了，它会在CPU上进行推理。运行下列命令，然后用户输入要问的问题，模型将给出回答。
```
main.exe -m dataroot/models/Qwen/Qwen-1_8B/ggml-model-q5_k_m.gguf -n 512 --chatml
```
运行结果见图7-1。

![image](https://github.com/user-attachments/assets/a6494c73-3070-4265-8400-2c691abb473d)


**图7-1 llama.cpp装载运行**
（此处有一个命令行窗口截图，内容为Python排序算法相关问答，因文字难以完整识别，未详细提取）

##### 7.2.6 Web方式运行

llama.cpp的server.exe工具提供了Web端的Chat服务。

首先，运行以下命令。
```
server.exe -m dataroot/models/Qwen/Qwen-1_8B/ggml-model-q5_k_m.gguf -c 2048
```
待服务正常运行后，在浏览器中打开http://127.0.0.1:8080，就可以在模型的WebUI中提问了。

#### 7.3 gemma.cpp量化过程

受到llama.cpp的启发，Google也为Gemma开发了一个gemma.cpp项目，用于Gemma基础模型量化。gemma.cpp是一个轻量级、独立的C++推理引擎。与llama.cpp的GGUF模型格式、GGML量化算法能广泛应用于各种模型不同，gemma.cpp专为Gemma的2B和7B模型定制开发，致力于使用上的简单和直接，而不考虑通用性。一般来说，使用C++部署的推理引擎在运行时和基于Python的训练框架之间在技术、原理、通用性方面存在很大的差别，所以gemma.cpp的运行过程与Transformers库的运行方式截然不同。

在本节的量化实例中，从https://www.kaggle.com下载已量化的2b-it模型，使其在Windows上使用CPU进行推理。其操作难点主要在于gemma.cpp需要根据操作系统环境进行编译，而它在Windows上对编译环境的要求较为苛刻。与llama.cpp将原始模型转化为量化模型的步骤不同，gemma.cpp提供了已量化模型的下载功能，所以可以将本节的目标理解为“如何使用gemma.cpp装载Gemma量化模型进行推理”。由于量化模型精度远低于原始模型，且运行在CPU上，所以其推理效果还有待提高。

##### 7.3.1 gemma.cpp源码下载
gemma.cpp在持续开发中，为了保证本例能够一直正常运行，建议在下载源码后，将其源码版本固定在某个版本（commit）上。这样不管以后gemma.cpp如何修改，都不会影响这个版本。源码下载方法如下。
git clone https://github.com/google/gemma.cpp
cd gemma.cpp
git checkout 0221956

##### 7.3.2 gemma.cpp编译
gemma.cpp应用于Windows时，操作系统版本需要是Windows10或Windows11，并且指定使用Visual Studio Build Tools 2022版本进行编译，该工具的安装选项见图7-2。与llama.cpp使用make方法编译不同，gemma.cpp使用cmake工具编译，且要由C++ Clang工具提供支持，过程较为复杂。
```
# 下载Visual Studio Build Tools 2022
https://aka.ms/vs/17/release/vs_BuildTools.exe
# 安装Visual Studio Build Tools
# 选择使用C++的桌面开发和适用于Windows的C++ Clang工具
# 设定环境变量path
# 将cmake.exe所在的目录(如c:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin)加到path路径中
# 配置build目录
cmake --preset windows 

```



# 第7章 大语言模型量化
## 7.3 gemma.cpp量化过程
受到llama.cpp的启发，Google也为Gemma开发了一个gemma.cpp项目，用于Gemma基础模型量化。gemma.cpp是一个轻量级、独立的C++推理引擎。与llama.cpp的GGUF模型格式、GGML量化算法能广泛应用于各种模型不同，gemma.cpp专为Gemma的2B和7B模型定制开发，致力于使用上的简单和直接，而不考虑通用性。一般来说，使用C++部署的推理引擎在运行时和基于Python的训练框架之间在技术、原理、通用性方面存在很大的差别，所以gemma.cpp的运行过程与Transformers库的运行方式截然不同。

在本节的量化实例中，从https://www.kaggle.com下载已量化的2b - it模型，使其在Windows上使用CPU进行推理。其操作难点主要在于gemma.cpp需要根据操作系统环境进行编译，而它在Windows上对编译环境的要求较为苛刻。与llama.cpp将原始模型转化为量化模型的步骤不同，gemma.cpp提供了已量化模型的下载功能，所以可以将本节的目标理解为“如何使用gemma.cpp装载Gemma量化模型进行推理”。由于量化模型精度远低于原始模型，且运行在CPU上，所以其推理效果还有待提高。

### 7.3.1 gemma.cpp源码下载
gemma.cpp在持续开发中，为了保证本例能够一直正常运行，建议在下载源码后，将其源码版本固定在某个版本（commit）上。这样不管以后gemma.cpp如何修改，都不会影响这个版本。源码下载方法如下。
```
git clone https://github.com/google/gemma.cpp
cd gemma.cpp
git checkout 0221956
```

### 7.3.2 gemma.cpp编译
gemma.cpp应用于Windows时，操作系统版本需要是Windows10或Windows11，并且指定使用Visual Studio Build Tools 2022版本进行编译，该工具的安装选项见图7 - 2。与llama.cpp使用make方法编译不同，gemma.cpp使用cmake工具编译，且要由C++ Clang工具提供支持，过程较为复杂。
- 下载Visual Studio Build Tools 2022
  - https://aka.ms/vs/17/release/vs_BuildTools.exe
- 安装Visual Studio Build Tools
  - 选择使用C++的桌面开发和适用于Windows的C++ Clang工具
  - 设定环境变量path
  - 将cmake.exe所在的目录(如c:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin)加到path路径中
  - 配置build目录
  - cmake --preset windows
- 使用Visual Studio Build Tools编译项目
  - 其中“ - j”后面的数字用CPU的核数，用于多核编译，提高编译效率
  - cmake --build --preset windows - j 4
- 编译完成
  - 编译的结果在build/Release/gemma.exe

![image](https://github.com/user-attachments/assets/344babe8-8e5a-4d95-83fd-a53c508def8a)


**图7 - 2 Visual Studio Build Tools 2022安装选项**
（此处有一张安装选项界面截图，因文字难以完整准确提取，未详细呈现内容 ）

### 7.3.3 量化模型下载
Gemma - 2B的量化版本要从https://www.kaggle.com/下载，地址如下。
```
https://www.kaggle.com/models/google/gemma/frameworks/gemmaCpp
```
下载前要注册kaggle用户，并填写调查表，写明模型用途。这是Gemma免费使用的前提条件。下载链接在“Gemma C++”选项卡中，选择2b - it - sfp后下载，如图7 - 3所示。下载的文件名为archive.tar.gz，可以用Winrar软件解压。

### 7.3.4 推理
将前面下载的模型文件archive.tar.gz解压，并将解压后的两个文件复制到models目录下。其中models\2b - it - sfp.sbs是压缩权重文件，models\tokenizer.spm是分词标记器文件。运行量化推理命令，如下所示。
```
build\Release\gemma.exe --tokenizer models\tokenizer.spm --compressed_weights models\2b - it - sfp.sbs --model 2b - it
```
运行结果如图7 - 4所示。从运行情况来看，Gemma模型发布的时间比较短，2B的模型规模也比较小，再加上量化后精度降低，该模型对于回答中文问题还面临一定的挑战，但它对于英文的支持效果良好。

![image](https://github.com/user-attachments/assets/67a5f0c9-1d9d-4c4e-af35-8f3e95f5f298)


**图7 - 3 Gemma - 2b - it - sfp下载**

（此处有一张kaggle网站模型下载界面截图，因文字难以完整准确提取，未详细呈现内容 ）

![image](https://github.com/user-attachments/assets/3dcb10a6-aae2-4303-8fe3-065ba4bf11c9)


**图7 - 4 Gemma量化模型推理**

（此处有一张命令行运行结果截图，内容为关于Gemma模型介绍等英文文本，因文字难以完整准确提取，未详细呈现内容 ） 
