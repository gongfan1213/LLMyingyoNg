### 第5章 大语言模型安装
有别于通过编程验证模型安装的方法，接下来实践一种新的方法，采用一种名为text-generation-webui的大模型Web操作工具来装载模型，进行推理测试，验证安装成果。text-generation-webui是基于Gradio开发的用于大语言模型的WebUI工具，能实现模型的多种方式装载、多种方式交互和微调，支持LLaMA、ChatGLM等大模型。

#### 5.3 LLaMA2安装
##### 5.3.2 Llama-2-7b-chat安装

（1）text-generation-webui安装

```bash
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
git checkout 1934cb6
# 创建Python3.10虚拟环境
conda create -n text-generation-webui python=3.10 -y
# 编辑requirements.txt，只保留tiktoken及其以前的库
# 因为后面的库本例用不到，全部安装会比较耗时
# 安装依赖库
conda activate text-generation-webui
pip install -r requirements.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
# 校验PyTorch
python -c "import torch; print(torch.cuda.is_available())"
```
（2）下载模型


```bash
# 从aliendao.cn下载Llama-2-7b-chat-hf模型文件
wget https://aliendao.cn/model_download.py
python model_download.py --e --repo_id NousResearch/Llama-2-7b-chat-hf \
 --token YPY8KHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/NousResearch/Llama-2-7b-chat-hf目录下
```

（3）部署模型

```bash
# 需要将text-generation-webui装载的模型连同文件夹一起复制到./models目录下
# 模型文件所在的目录(如Llama-2-7b-chat-hf)会显示在WebUI中供用户选择
mkdir./models/Llama-2-7b-chat-hf
mv./dataroot/models/NousResearch/Llama-2-7b-chat-hf/* \
./models/Llama-2-7b-chat-hf/
```
##### 5.3.3 运行验证
运行以下命令，启动text-generation-webui服务。
```bash
python server.py --listen-host 0.0.0.0 --listen
```
Llama-2-7b-chat-hf模型需要约13GB的GPU内存来进行装载，如果GPU内存不够，则可在命令后面跟“--load-in-4bit”参数进行量化装载，这样大约需要不到5GB的GPU内存就可运行。如果推理卡支持，也可以用“--load-in-8bit”参数，这样约占用9GB的GPU内存。

注意：并非所有支持4bit的推理卡都支持8bit。

![image](https://github.com/user-attachments/assets/ec888e29-5c53-44e5-afb7-c44192cf4326)


运行结果如图5-5所示。

在浏览器中打开http://server-llm-dev:7080，然后进入Model选项卡，从Model列表中选择Llama-2-7b-chat-hf，再单击“Load”按钮。若提示“Successfully loaded Llama-2-7b-chat-hf”，则表示装载完成，见图5-6。

![image](https://github.com/user-attachments/assets/5d2b439c-556c-468d-bbb1-de5a2dcde304)


然后在Chat选项卡中进行提问，结果如图5-7所示。可知LLaMA2可以理解中文问题，但其回答是中英文混杂的，效果一般。

![image](https://github.com/user-attachments/assets/4c522ea6-a174-495a-8070-525570c529b0)


为了实现对中文的更好支持，可以尝试使用一些LLaMA2的中文语料微调版本，如LinkSoul/Chinese-Llama-2-7b。将其按下述方法下载、部署、装载后进行测试，发现其中文效果明显好于原始模型。操作方法简述如下。


```bash
# 下载LinkSoul/Chinese-Llama-2-7b模型
python model_download.py --e --repo_id LinkSoul/Chinese-Llama-2-7b \
 --token YPY8KHDQ2NAHQ2SG
# 部署模型
mkdir./models/Chinese-Llama-2-7b
mv./dataroot/models/LinkSoul/Chinese-Llama-2-7b/* \
./models/Chinese-Llama-2-7b/
# 启动服务后装载模型并测试
python server.py --listen-host 0.0.0.0 --listen
```

#### 5.4 Gemma安装
##### 5.4.1 Gemma模型介绍
Gemma是一系列轻量级的开放大语言模型，由Google的团队开发，采用了与Gemini模型（由Google推出的对标GPT-4的大模型）相同的研究和技术基础。Gemma系列模型是从文本到文本的、基于Decoder-only架构的大语言模型，提供英语版本，具有开放式权重、预训练变体和指令调优变体。除了发布模型权重之外，Google还提供了模型相关的推理、量化和微调的工具，通过Hugging Face面向更广泛的开源社区开放。目前Gemma提供了针对20亿（2B）和70亿（7B）参数规模的预训练版本，同时提供了经过指令调优的版本。

为了确保Gemma的预训练模型安全可靠，Google使用自动化技术在训练阶段集中过滤掉了特定的个人信息和其他敏感数据，进行了广泛的微调和来自人类反馈的强化学习，以使微调模型与人类行为对齐。并且，为了减少Gemma模型的风险，Google进行了全面的评估，包括手动测试、自动对抗测试以及对模型进行危险活动评估。

如表5-2所示，Gemma目前提供了2B和7B两种规模的版本，每种又分别包含了预训练基础版本和经过指令优化的版本。Gemma的所有版本均可在各类消费级硬件或推理卡上运行，如使用RTX4090、RTX3090、P100、T4等，也可在不对模型进行量化处理的情况下正常装载运行，并且拥有高达8000个token的处理能力。

**表5-2 Gemma模型版本**


|模型名称|版本说明|
| ---- | ---- |
|Gemma-7B|70亿参数的基础模型|
|Gemma-7B-it|70亿参数的指令优化模型|
|Gemma-2B|20亿参数的基础模型|
|Gemma-2B-it|20亿参数的指令优化模型|

Gemma的原始模型首先在kaggle.com上发布。并且，Google与Hugging Face合作，将模型集成到了Hugging Face的生态系统中，使原始模型被转化成hf格式供开发者免费下载。Gemma的hf格式模型可以通过Hugging Face的Transformers库装载运行，这与其他模型（如LLaMA2等）在使用方面没有太大区别。对于模型的推理过程，可以使用Transformers库的AutoModelForCausalLM.from_pretrained方法装载模型，再用model.generate方法进行模型推理。装载模型时，用load_in_8bit和load_in_4bit量化模式会非常顺利，占用资源也比较少。

在提示词方面，Gemma的基础版本模型没有特殊要求，但指令优化版本（以“-it”结尾的模型进行推理时，需要提示词遵循以下格式。
```
<start_of_turn>user
问题<end_of_turn>
<start_of_turn>model
模型回答<end_of_turn>
```
上述要求只是基于原始模型的。如果采用hf格式模型的话，那么Transformers库会将“-it”和“非-it”的模型推理提示词都封装成一样的格式。

实际测试显示，Gemma对中文的支持情况并不尽如人意，但从开发、训练的角度来看，Gemma作为一种除LLaMA2以外的可用的基础模型和技术体系，也是值得学习的。

##### 5.4.2 Gemma-2B安装
（1）建立Python3.10虚拟环境并激活
```bash
conda create -n gemma python=3.10 -y
conda activate gemma
```
（2）建立测试目录
```bash
mkdir gemma-2b
cd gemma-2b
```
（3）建立依赖库文件

建立requirements.txt文件，内容如下。
```
torch==2.0.1
transformers==4.38.1
accelerate==0.27.2
bitsandbytes==0.42.0
trl==0.7.11
peft==0.8.2
```
（4）安装依赖库
```bash
pip install -r requirements.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
```
（5）下载模型
```bash
# 从aliendao.cn下载gemma-2b模型文件
wget https://aliendao.cn/model_download.py
python model_download.py --e --repo_id alpindale/gemma-2b \
 --token YPY8KHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/alpindale/gemma-2b目录下
```
##### 5.4.3 编程验证
编写Python程序gemma-inference.py，该程序能实现：用户输入问题，模型给出回答，按下回车键退出。文件内容如下。
```python
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM, \
 BitsAndBytesConfig

def generate(load_in_8bit, load_in_4bit):
    model_path = "./dataroot/models/alpindale/gemma-2b"
    quantization_config = None
    if load_in_8bit:
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
    if load_in_4bit:
        quantization_config = BitsAndBytesConfig(load_in_4bit=True)

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if quantization_config is None:
        model = AutoModelForCausalLM.from_pretrained(
            model_path, device_map="auto")
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_path, quantization_config=quantization_config)
    while True:
        prompt = input("请输入您的问题：")
        if prompt == "":
            break
        input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(*input_ids)
        print(tokenizer.decode(outputs[0]))

if __name__ == "__main__":
    print("torch cuda:", torch.cuda.is_available())
    parser = argparse.ArgumentParser()
    parser.add_argument('--load_in_8bit', default=False,
                        action='store_true', required=False)
    parser.add_argument('--load_in_4bit', default=False,
                        action='store_true', required=False)
    args = parser.parse_args()
    generate(args.load_in_8bit, args.load_in_4bit)
```
运行python gemma-inference.py进行测试。可以用全精度、4bit量化和8bit量化三种模式装载运行该模型。
```bash
# 全精度
python gemma-inference.py
# 8bit量化
python gemma-inference.py --load_in_8bit
# 4bit量化
python gemma-inference.py --load_in_4bit
```
模型推理效果见图5-8。

![image](https://github.com/user-attachments/assets/090af0a5-e4bb-4d72-9939-e8c03341b44b)


#### 5.5 Whisper安装
##### 5.5.1 Whisper-large-v3介绍
Whisper是OpenAI开发的一种用于自动语音识别（ASR）和语音翻译的预训练模型，于2022年12月发布，该研究成果的论文为“Robust Speech Recognition via Large-Scale Weak Supervision” 。模型除了具有语音识别能力，还具备语音活性检测（VAD）、声纹识别、语音翻译（将其他语言的语音翻译为英语）等功能。

Whisper是一种基于Transformer的编码器－解码器模型，也称为序列到序列模型。它使用了100万h的弱标记音频和400万h的伪标记音频进行训练，这些音频是使用Whisper large-v2收集的。Whisper系列模型中有的采用纯英语数据进行训练，有的采用多语言数据进行训练。纯英语模型主要接受语音识别任务的训练，而多语言模型在语音识别和语音翻译方面进行了训练。对于语音识别，模型能预测与音频相同语言的转录内容；对于语音翻译，模型能预测与音频不同语言的转录内容。Whisper检查点有五种不同规模的配置，较小规模的四种是在英语、多语言数据上训练的，大规模的检查点（large、large-v2和large-v3）只在多语言数据上训练。详细情况见表5-3。

**表5-3 Whisper模型参数与语言训练情况**


|型号|参数量|纯英语|多语言|
| ---- | ---- | ---- | ---- |
|tiny|3900万|√|√|
|base|7400万|√|√|
|small|2.44亿|√|√|
|medium|7.69亿|√|√|
|large|15.5亿|×|√|
|large-v2|15.5亿|×|√|
|large-v3|15.5亿|×|√|

Whisper-large-v3是Whisper的大规版本，包含15.5亿个参数，具有很强的中文语音识别能力，也能很好地支持粤语。Whisper-large-v3模型文件共23GB左右，但对GPU的要求并不高，装载模型大约需要4GB的GPU内存。

##### 5.5.2 Whisper-large-v3安装
（1）创建虚拟环境
```bash
conda create -n whisper-large-v3 python=3.10 -y
conda activate whisper-large-v3
```
（2）建立测试目录
```bash
mkdir Whisper-large-v3
cd Whisper-large-v3
```
（3）建立依赖库文件
建立requirements_whisper.txt文件，内容如下。
```
torch==2.0.1
transformers==4.38.1
accelerate==0.28.0
pydub==0.25.1
```
（4）安装依赖库
```bash
pip install -r requirements_whisper.txt \
 -i https://pypi.mirrors.ustc.edu.cn/simple \
 --trusted-host=pypi.mirrors.ustc.edu.cn
# 校验PyTorch
python -c "import torch; print(torch.cuda.is_available())"
```
（5）下载模型
```bash
# 从aliendao.cn下载Whisper-large-v3模型文件
wget https://aliendao.cn/model_download.py
python model_download.py --e --repo_id openai/whisper-large-v3 \
 --token YPY8KHDQ2NAHQ2SG
# 下载后的文件在./dataroot/models/openai/whisper-large-v3目录下
```
##### 5.5.3 编程验证
编写Python程序whisper-audio.py，以将Windows录音程序录制的test.m4a音频识别成中文文本。Whisper支持.wav、.mp3和.flac等格式的文件，不支持.m4a格式，所以要先使用pydub库将test.m4a转成.mp3格式，再调用模型进行识别。pydub依赖的FFmpeg需要预先安装。FFmpeg的安装方法如下。
```bash
# Ubuntu
sudo apt install ffmpeg
# Redhat
sudo yum install ffmpeg ffmpeg-devel
# Windows，下载后解压将ffmpeg.exe放到whisper-audio.py同级目录下
https://wwwgyan.dev/ffmpeg/builds/
```
whisper-audio.py的源码如下。
```python
import torch
from transformers import AutoModelForSpeechSeq2Seq, \
 AutoProcessor, pipeline
from pydub import AudioSegment

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 \
    if torch.cuda.is_available() else torch.float32
model_id = "./dataroot/models/openai/whisper-large-v3"

def trans_m4a_to_mp3(m4a_file, mp3_file):
    song = AudioSegment.from_file(m4a_file)
    song.export(mp3_file, format='mp3')

if __name__ == "__main__":
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id, torch_dtype=torch_dtype,
        low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)

    processor = AutoProcessor.from_pretrained(model_id)

    pipe = pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        max_new_tokens=128,
        chunk_length_s=30,
        batch_size=16,
        return_timestamps=True,
        torch_dtype=torch_dtype,
        device=device,
    )
    trans_m4a_to_mp3("test.m4a", "test.mp3")
    result = pipe("test.mp3")
    print(result["text"])
```
运行“python whisper-audio.py”命令进行测试，结果见图5-9。 


![image](https://github.com/user-attachments/assets/26b60b20-ded9-4fb9-99ba-7678205681bb)
